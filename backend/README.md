# Backend (Gemini LLM)

This folder now exposes a FastAPI service that ingests PDF files, rewrites the extracted text according to a NeuroClear support mode, and returns structured JSON generated by Google Gemini.

## Prerequisites

1. Install dependencies inside your virtual environment:

		```bash
		pip install -r requirements.txt
		```

2. Provide a Gemini API key (replace the placeholder with your key) by copying `.env.example` to `.env` and editing the value:

	```bash
	cp backend/.env.example backend/.env  # adjust command for PowerShell if needed
	```

	Optionally persist the key globally (PowerShell):

		```powershell
		setx GEMINI_API_KEY "your-key"
		```

	The backend automatically loads `backend/.env` on startup; a global variable is only necessary if you deploy elsewhere.

## Run the API locally

```bash
uvicorn backend.app:app --reload
```

The service exposes:

- `GET /health` – simple readiness probe.
- `POST /api/v1/pdf-transform` – accepts a `mode` form field (`adhd`, `autism`, `dyslexia`, `anxiety`, or `elderly`) plus a `pdf` upload. The backend extracts the PDF text, builds a mode-specific prompt, calls Gemini, and returns JSON such as:

```json
{
	"status": "ok",
	"mode": "adhd",
	"pages": 2,
	"characters": 1850,
	"result": {
		"rewrite": "...",
		"highlights": ["..."],
		"reading_time_minutes": 3
	}
}
```

Example request:

```bash
curl -X POST \
	-F "mode=adhd" \
	-F "pdf=@sample.pdf" \
	http://localhost:8000/api/v1/pdf-transform
```

## CLI helper (optional)

The original CLI is still available for quick manual prompts:

```bash
python main.py "Write a haiku about code reviews."
```

Add `--no-stream` to print the full response after completion instead of streaming chunks.

## Container image

Build and run the backend container locally (Docker Desktop):

```bash
docker build -f backend/Dockerfile -t neuroclear-backend ..
docker run --rm -p 8080:8080 --env-file backend/.env neuroclear-backend
```

For Cloud Run deployment, see `DEPLOYMENT.md` or use `scripts/deploy_cloud_run.sh`.
